{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/ub_dlt_demo_pipeline.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import DLT and src/ub_dlt_demo\n",
    "import dlt\n",
    "import sys\n",
    "sys.path.append(spark.conf.get(\"bundle.sourcePath\", \".\"))\n",
    "import pyspark.sql.functions as F\n",
    "from ub_dlt_demo import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting variables to identify the landing zone location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account = \"ubsadatabrickspocnpl2\"\n",
    "storage_container = \"umpquapocdev\"\n",
    "lz_base_path = \"umpqua_poc/landing_zone\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data From Landing Zone\n",
    "\n",
    "#### DLT Decorator Function (Invokes DLT & Adds Metadata to Table)\n",
    "- Table name set using the \"name=\" notation in the @dlt.table() decorator\n",
    "- Comment added to UC Table in the @dlt.table() decorator\n",
    "- Table Properties added using the \"table_properties={}\" notation in the @dlt.table() decorator\n",
    "\n",
    "#### Table Function (Defines logic used by DLT for the table generated)\n",
    "- Includes the type of data source. \n",
    "    - The \"readStream\" denotes that we will be performing incremental ingestion\n",
    "    - In this case we use the \"cloudfiles\" format to denote ingestion from object storage w/ AutoLoader\n",
    "- Numerous options are specified to modify the way data is handled:\n",
    "    - The format of the data files is specified as \"csv\"\n",
    "    - The data contains headers specified with \"header\": \"true\"\n",
    "    - The data delimiter is set with \"delimiter\": \"||\"\n",
    "    - The column name for rescued data is specified with \"rescuedDataColumn\": \"_rescued_data\"\n",
    "    - A check is performed to ensure all specified options are valid with \"cloudFiles.validateOptions\": \"true\"\n",
    "    - Directory listing v. File Notification method is specified with \"cloudFiles.useNotifications\": \"false\"\n",
    "    - Backfill interval is specified with \"cloudFiles.backfillInterval\": \"1 day\"\n",
    "    - Schema Evolution Mode is set to Rescue with \"cloudFiles.schemaEvolutionMode\": \"rescue\"\n",
    "    - Processing of overwritten files is ignored with \"cloudFiles.allowOverwrites\": \"false\"\n",
    "- The path to the data source is set\n",
    "    - f\"abfss://{storage_container}@{storage_account}.dfs.core.windows.net/{lz_base_path}/customerpiidata\"\n",
    "    - A storage container, storage account, and landing zone path are set with variables declared previously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"customerpiidata\",\n",
    "    comment=\"Raw custom data capture for customerpiidata\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "    },\n",
    ")\n",
    "def customerpiidata_dlt():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .options(\n",
    "            **{\n",
    "                \"cloudFiles.format\": \"csv\",\n",
    "                \"header\": \"true\",\n",
    "                \"delimiter\": \"||\",\n",
    "                \"rescuedDataColumn\": \"_rescued_data\",\n",
    "                \"cloudFiles.validateOptions\": \"true\",\n",
    "                \"cloudFiles.useNotifications\": \"false\",\n",
    "                \"cloudFiles.inferColumnTypes\": \"true\",\n",
    "                \"cloudFiles.backfillInterval\": \"1 day\",\n",
    "                \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n",
    "                \"cloudFiles.allowOverwrites\": \"false\",\n",
    "            }\n",
    "        )\n",
    "        .load(\n",
    "            f\"abfss://{storage_container}@{storage_account}.dfs.core.windows.net/{lz_base_path}/customerpiidata\"\n",
    "        )\n",
    "    )\n",
    "abfss://umpquapocdev@ubsadatabrickspocnpl2.dfs.core.windows.net/umpqua_poc/landing_zone/customerpiidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further process raw bronze data to silver\n",
    "\n",
    "#### Additional DLT Decorator Is Added\n",
    "- The @dlt.expect_or_drop() decorator is used to enforce data quality standards on the silver level\n",
    "\n",
    "#### Table Function (Defines logic used by DLT for the table generated)\n",
    "- The data source is changed from the previous example\n",
    "    - Instead of using AutoLoader to ingest data from object storage, we will be using a Delta Table as the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"customerpiidata_clean\",\n",
    "    comment=\"Cleaned customerpiidata\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "    },\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_tax_id\", \"tax_id IS NOT NULL\")\n",
    "def customerpiidata_clean_dlt():\n",
    "    return dlt.read_stream(\"customerpiidata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further process silver data to 2x gold tables\n",
    "\n",
    "#### Table Function (Defines logic used by DLT for the table generated)\n",
    "- Delta Tables are read & filtered to create gold tables for a specific business purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"corporate_customer_data\",\n",
    "    comment=\"Clean corporate customer data\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "    },\n",
    ")\n",
    "def corporate_customer_data_dlt():\n",
    "    return dlt.read_stream(\"customerpiidata_clean\").filter(F.col(\"is_company\") == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"consumer_customer_data\",\n",
    "    comment=\"Clean consumer customer data\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "    },\n",
    ")\n",
    "def consumer_customer_data_dlt():\n",
    "    return dlt.read_stream(\"customerpiidata_clean\").filter(F.col(\"is_company\") == 0)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
